"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.handlebarsLoaderForVitest = exports.shutdownLangfuse = exports.langfuseInstance = exports.getPrompt = exports.flushAllTraces = exports.compilePrompt = exports.LLM = void 0;
const async_retry_1 = __importDefault(require("async-retry"));
const openai_1 = __importDefault(require("openai"));
const portkey_ai_1 = require("portkey-ai");
const lib_1 = require("./prompts/lib");
Object.defineProperty(exports, "compilePrompt", { enumerable: true, get: function () { return lib_1.compilePrompt; } });
const provider_1 = require("./prompts/provider");
Object.defineProperty(exports, "getPrompt", { enumerable: true, get: function () { return provider_1.getPrompt; } });
const trace_1 = require("./trace");
Object.defineProperty(exports, "flushAllTraces", { enumerable: true, get: function () { return trace_1.flushAllTraces; } });
Object.defineProperty(exports, "langfuseInstance", { enumerable: true, get: function () { return trace_1.langfuseInstance; } });
Object.defineProperty(exports, "shutdownLangfuse", { enumerable: true, get: function () { return trace_1.shutdownLangfuse; } });
class LLM {
    _trace;
    _provider;
    _providerApiKey;
    _usedTokens = 0;
    _defaultModel;
    // limit the max tokens to avoid infinite or high number of roundtrips to llm
    _maxTokens;
    completionTokens = 0;
    promptTokens = 0;
    constructor({ trace, provider, providerApiKey, maxTokens, defaultModel, }) {
        this._trace = trace;
        this._provider = provider;
        this._providerApiKey = providerApiKey;
        this._maxTokens = maxTokens ?? 1000000;
        this._defaultModel = defaultModel;
    }
    async createChatCompletion({ messages, modelParameters, model, tools, trace, responseFormat, traceName = "get-llm-result", }) {
        if (this._usedTokens >= this._maxTokens) {
            throw new Error(`Exceeded max tokens limit of ${this._maxTokens} tokens. Please try again later.`);
        }
        const openai = new openai_1.default({
            apiKey: this._providerApiKey,
            baseURL: "https://ai.empirical.run/v1/",
            defaultHeaders: (0, portkey_ai_1.createHeaders)({
                provider: this._provider,
            }),
        });
        // if model is not provided, use the default model
        model = model || this._defaultModel;
        const generation = (trace || this._trace)?.generation({
            name: traceName,
            model,
            modelParameters,
            input: {
                messages,
                tools,
            },
        });
        try {
            const completion = await (0, async_retry_1.default)(async (bail) => {
                try {
                    const response = await openai.chat.completions.create({
                        messages,
                        model: model,
                        tools,
                        max_completion_tokens: 1000,
                        ...modelParameters,
                        stream: false,
                        response_format: responseFormat,
                    });
                    return response;
                }
                catch (err) {
                    if (err instanceof openai_1.default.APIError &&
                        err.status &&
                        [400, 401, 403].includes(err.status)) {
                        bail(err);
                        return;
                    }
                    else {
                        throw err;
                    }
                }
            }, {
                retries: 5,
                factor: 3,
                minTimeout: 1000,
                maxTimeout: 60000,
                randomize: true,
            });
            const output = completion?.choices[0]?.message;
            if (completion && !completion.usage?.total_tokens) {
                console.warn("No usage.total_tokens in completion");
            }
            this.completionTokens = completion?.usage?.completion_tokens || 0;
            this.promptTokens = completion?.usage?.prompt_tokens || 0;
            this._usedTokens += completion?.usage?.total_tokens || 0;
            generation?.end({
                output,
                usage: this.completionTokens && this.promptTokens
                    ? {
                        input: this.promptTokens,
                        output: this.completionTokens,
                        unit: "TOKENS",
                    }
                    : undefined,
            });
            return output;
        }
        catch (err) {
            generation?.end({
                output: {
                    error: true,
                    message: err.message || err.toString(),
                },
            });
            await (0, trace_1.flushAllTraces)();
            throw err;
        }
    }
}
exports.LLM = LLM;
var vitest_plugin_1 = require("./prompts/lib/vitest-plugin");
Object.defineProperty(exports, "handlebarsLoaderForVitest", { enumerable: true, get: function () { return vitest_plugin_1.handlebarsLoader; } });
