import { LangfuseGenerationClient, LangfuseSpanClient, LangfuseTraceClient } from "langfuse";
import OpenAI from "openai";
import { compilePrompt } from "./prompts/lib";
import { getPrompt } from "./prompts/provider";
import { flushAllTraces, langfuseInstance, shutdownLangfuse } from "./trace";
import { LLMModel, LLMProvider, ModelParameters } from "./types";
type TraceClient = LangfuseTraceClient | LangfuseSpanClient;
export declare class LLM {
    private _trace;
    private _provider;
    private _providerApiKey;
    private _usedTokens;
    private _defaultModel;
    private _maxTokens;
    completionTokens: number;
    promptTokens: number;
    constructor({ trace, provider, providerApiKey, maxTokens, defaultModel, }: {
        trace?: TraceClient;
        provider: LLMProvider;
        providerApiKey?: string;
        maxTokens?: number;
        defaultModel?: LLMModel;
    });
    createChatCompletion({ messages, modelParameters, model, tools, trace, responseFormat, traceName, }: {
        tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
        messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
        modelParameters?: LangfuseGenerationClient["generation"]["arguments"]["modelParameters"];
        model?: LLMModel;
        trace?: TraceClient;
        traceName?: string;
        responseFormat?: OpenAI.ChatCompletionCreateParamsNonStreaming["response_format"];
    }): Promise<OpenAI.Chat.Completions.ChatCompletionMessage | undefined>;
}
export type { TraceClient };
export { compilePrompt, flushAllTraces, getPrompt, langfuseInstance, shutdownLangfuse, };
export type { LLMModel, LLMProvider, ModelParameters };
export { handlebarsLoader as handlebarsLoaderForVitest } from "./prompts/lib/vitest-plugin";
//# sourceMappingURL=index.d.ts.map